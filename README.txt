Welcome to the mashup of grading scripts that are still a work in progress, here is the current directions.
All .py files should be run with python3.

Pre-reqs and Structure:
Each grading folder should have a few things in it:
Grading_Folder/
    solutions/
    submissions/
    tests/
    grading-script.py
    prepare-solutions.py
    <lab-solution>.py
    manual-grading.py
    README.txt
    results.csv

Files in solutions/ will be generated by prepare-solutions.py
Files in submissions/ should be downloaded from the assignment page on canvas.
Files in tests/ should be written by the grader, they will provide the input for the submissions/.
<lab-solution>.py should be a correct solution that will generate the expected solutions.
results.csv will be generated by the grading-script.py


Known issues:
Doesn't handle floats when ints are expected and vice versa.
Format has to be exact.

Manual Grading:
Manual grading runs all of the labs successively. After the lab is run it asks for a grade and comments, grade should be out of the number of points for the assignment. For the Grade field, "s" or <ENTER> skips to the next lab, "q" quits and writes to the csv, error in this python runner will quit and write to the csv. 

One method right now, depending on the lab, is to have the speed grader open on half the screen and a powershell or command line on the other side, that way you can run the labs in the command line (with manual-grading), have their code and be able to enter grades and comments in the speed grader.

Notes for manual grading:
Linux os requires a slightly modified file.
For Linux: Shell=True necessary in Popen statement, also the text parameter should maybe be removed.
List of submissions must be sorted prior to use.